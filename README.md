# AIS Data Processing Framework (AIS-DSTGT)

**Comprehensive AIS Data Processing with Advanced Trajectory Analysis**

[![CI](https://github.com/EvanYzl/AIS_D-STGT/workflows/CI/badge.svg)](https://github.com/EvanYzl/AIS_D-STGT/actions)
[![codecov](https://codecov.io/gh/EvanYzl/AIS_D-STGT/branch/main/graph/badge.svg)](https://codecov.io/gh/EvanYzl/AIS_D-STGT)
[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3120/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## Overview

AIS-DSTGT is a comprehensive framework for processing and analyzing Automatic Identification System (AIS) maritime data. It provides advanced preprocessing capabilities including trajectory smoothing, behavior-based segmentation, scene aggregation, and collision risk assessment for deep learning applications in maritime domain.

## Key Features

- **Data Ingestion**: Robust CSV file handling with validation and error recovery
- **Advanced Preprocessing**: Multi-layer data cleaning, coordinate transformation, and anomaly detection
- **Trajectory Smoothing**: Enhanced Kalman filtering with RTS smoothing and gap interpolation
- **Trajectory Segmentation**: Behavior-based segmentation with port visit detection and voyage partitioning
- **Scene Aggregation**: Multi-vessel interaction analysis and navigation scene classification
- **Risk Assessment**: TCPA/DCPA calculations for collision risk evaluation
- **PyTorch Integration**: Custom dataset classes for efficient deep learning model training
- **Production Ready**: Docker support with comprehensive configuration options

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/EvanYzl/AIS_D-STGT.git
cd AIS_D-STGT

# Install with Poetry (recommended)
poetry install

# Or install with pip
pip install -e .
```

### Basic Usage

```python
from ais_dstgt.data.processor import AISDataProcessor

# Initialize processor with configuration
processor = AISDataProcessor()

# Load and process AIS data
df = processor.load_data('path/to/ais_data.csv')
processed_data = processor.process_data(df)

# Access processed components
smoothed_trajectories = processed_data['smoothed_trajectories']
trajectory_segments = processed_data['trajectory_segments']
scene_analysis = processed_data['scene_analysis']
risk_assessment = processed_data['risk_assessment']
```

### Docker Usage

```bash
# Build and run with Docker Compose
docker-compose up -d

# Or build manually
docker build -t ais-dstgt .
docker run --gpus all -p 8000:8000 ais-dstgt
```

## Architecture

```
ais_dstgt/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ingestion/           # Data loading and parsing
â”‚   â”œâ”€â”€ preprocessing/       # Core preprocessing modules
â”‚   â”‚   â”œâ”€â”€ trajectory_segmenter.py      # Trajectory segmentation
â”‚   â”‚   â”œâ”€â”€ trajectory_smoother.py       # Enhanced smoothing
â”‚   â”‚   â”œâ”€â”€ scene_aggregator.py          # Scene analysis
â”‚   â”‚   â”œâ”€â”€ tcpa_dcpa_calculator.py      # Risk assessment
â”‚   â”‚   â”œâ”€â”€ scene_dataset.py             # PyTorch dataset
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ validation/          # Data quality validation
â”‚   â””â”€â”€ processor.py         # Main processing pipeline
```

## Preprocessing Modules

### 1. Enhanced Trajectory Smoother (`trajectory_smoother.py`)

The trajectory smoother implements advanced Kalman filtering with Rauch-Tung-Striebel (RTS) smoothing for optimal trajectory estimation.

**Key Features:**
- Local coordinate projection using Transverse Mercator
- Multi-layer outlier detection (speed, acceleration, statistical)
- Gap interpolation using physics-based predictions
- Quality metrics and uncertainty estimation

**Code Reference:**
```python
# Main smoothing pipeline (lines 120-180)
def smooth_trajectory(self, df: pd.DataFrame) -> SmoothingResult:
    # Project to local coordinates
    projected_data = self._project_to_local_coordinates(df_sorted)

    # Detect and remove outliers
    cleaned_data, outliers_removed = self._detect_and_remove_outliers(projected_data)

    # Fill gaps with interpolation
    filled_data, gaps_filled = self._fill_gaps(cleaned_data)

    # Apply Kalman filtering
    filtered_states, filtered_covariances = self._apply_kalman_filter(filled_data)

    # Apply RTS smoothing
    smoothed_states, smoothed_covariances = self._apply_rts_smoothing(
        filtered_states, filtered_covariances, filled_data
    )
```

### 2. Trajectory Segmentation (`trajectory_segmenter.py`)

The trajectory segmenter provides comprehensive trajectory partitioning based on maritime behavior patterns.

#### æ—¶é—´åˆ’åˆ†ç­–ç•¥ (Time Division Strategies)

**1. åŸºäºè¡Œä¸ºçš„åˆ†æ®µ (Behavior-Based Segmentation)**
- **é€Ÿåº¦å˜åŒ–æ£€æµ‹**: å½“é€Ÿåº¦å˜åŒ–è¶…è¿‡é˜ˆå€¼æ—¶åˆ†æ®µè½¨è¿¹
- **èˆªå‘å˜åŒ–æ£€æµ‹**: è¯†åˆ«æ˜¾è‘—çš„èˆªå‘å˜åŒ– (>30Â°)
- **æ—¶é—´çª—å£**: å¯é…ç½®çš„åˆ†æçª—å£ (é»˜è®¤: 300ç§’)

```python
# é€Ÿåº¦å˜åŒ–åˆ†æ®µ (lines 180-200)
def _detect_speed_changes(self, df: pd.DataFrame) -> List[int]:
    speed_changes = []
    for i in range(1, len(df)):
        speed_change = abs(df.iloc[i]['speed'] - df.iloc[i-1]['speed'])
        if speed_change > self.speed_change_threshold:
            speed_changes.append(i)
    return speed_changes

# èˆªå‘å˜åŒ–åˆ†æ®µ (lines 202-220)
def _detect_course_changes(self, df: pd.DataFrame) -> List[int]:
    course_changes = []
    for i in range(1, len(df)):
        course_change = self._calculate_course_change(
            df.iloc[i-1]['course'], df.iloc[i]['course']
        )
        if course_change > self.course_change_threshold:
            course_changes.append(i)
    return course_changes
```

**2. æ¸¯å£è®¿é—®æ£€æµ‹ (Port Visit Detection)**
- **åŸºäºé‚»è¿‘åº¦**: æ£€æµ‹èˆ¹èˆ¶ä½•æ—¶åœ¨æ¸¯å£è¾¹ç•Œå†…
- **åŸºäºé€Ÿåº¦**: è¯†åˆ«è¡¨ç¤ºæ¸¯å£æ´»åŠ¨çš„ä½é€ŸæœŸ
- **æŒç»­æ—¶é—´é˜ˆå€¼**: æœ€å°åœç•™æ—¶é—´ (é»˜è®¤: 1800ç§’)

```python
# æ¸¯å£è®¿é—®æ£€æµ‹ (lines 250-290)
def _detect_port_visits(self, df: pd.DataFrame) -> List[Dict]:
    port_visits = []

    for port_name, port_location in self.port_locations.items():
        # æ£€æŸ¥ä¸æ¸¯å£çš„é‚»è¿‘åº¦
        distances = df.apply(lambda row: geodesic(
            (row['latitude'], row['longitude']), port_location
        ).meters, axis=1)

        # æ‰¾åˆ°æ¸¯å£åŠå¾„å†…çš„è¿ç»­æ—¶æœŸ
        in_port = distances <= self.port_radius
        port_periods = self._find_continuous_periods(in_port)

        for start_idx, end_idx in port_periods:
            duration = (df.iloc[end_idx]['timestamp'] -
                       df.iloc[start_idx]['timestamp']).total_seconds()

            if duration >= self.min_port_duration:
                port_visits.append({
                    'port_name': port_name,
                    'start_index': start_idx,
                    'end_index': end_idx,
                    'duration': duration
                })
```

**3. èˆªæ¬¡åˆ†å‰² (Voyage Partitioning)**
- **æ¸¯å£é—´æ®µ**: åœ¨æ¸¯å£è®¿é—®ä¹‹é—´åˆ’åˆ†è½¨è¿¹
- **å¼€é˜”æµ·åŸŸæ®µ**: è¯†åˆ«è¿ç»­çš„å¯¼èˆªæœŸ
- **æ—¶é—´é—´éš™å¤„ç†**: åŸºäºæ—¶é—´ä¸è¿ç»­æ€§çš„åˆ†æ®µ

```python
# èˆªæ¬¡åˆ†å‰² (lines 320-360)
def _partition_voyage(self, df: pd.DataFrame, port_visits: List[Dict]) -> List[Dict]:
    voyage_segments = []

    # æŒ‰å¼€å§‹æ—¶é—´æ’åºæ¸¯å£è®¿é—®
    port_visits.sort(key=lambda x: x['start_index'])

    # åœ¨æ¸¯å£ä¹‹é—´åˆ›å»ºæ®µ
    for i in range(len(port_visits) - 1):
        current_port = port_visits[i]
        next_port = port_visits[i + 1]

        # ä»å½“å‰æ¸¯å£ç¦»å¼€åˆ°ä¸‹ä¸€ä¸ªæ¸¯å£åˆ°è¾¾çš„èˆªæ¬¡æ®µ
        voyage_start = current_port['end_index']
        voyage_end = next_port['start_index']

        if voyage_end > voyage_start:
            voyage_segments.append({
                'type': 'voyage',
                'start_index': voyage_start,
                'end_index': voyage_end,
                'origin_port': current_port['port_name'],
                'destination_port': next_port['port_name']
            })
```

**4. å¯¼èˆªçŠ¶æ€æ£€æµ‹ (Navigation State Detection)**
- **é”šæ³Š**: é€Ÿåº¦ < 0.5èŠ‚ æŒç»­ > 30åˆ†é’Ÿ
- **èˆªè¡Œ**: é€Ÿåº¦ > 3èŠ‚ ä¸”èˆªå‘ä¸€è‡´
- **æ“çºµ**: é¢‘ç¹èˆªå‘å˜åŒ–ä¸”é€Ÿåº¦å¯å˜
- **é æ³Š**: æ¸¯å£è®¾æ–½é™„è¿‘çš„æä½é€Ÿåº¦

```python
# å¯¼èˆªçŠ¶æ€æ£€æµ‹ (lines 380-420)
def _detect_navigation_states(self, df: pd.DataFrame) -> List[Dict]:
    states = []

    for i in range(len(df)):
        speed = df.iloc[i]['speed']

        if speed < 0.5:  # é”šæ³Šé˜ˆå€¼
            # æ£€æŸ¥ä½é€ŸæŒç»­æ—¶é—´
            low_speed_duration = self._calculate_low_speed_duration(df, i)
            if low_speed_duration > 1800:  # 30åˆ†é’Ÿ
                states.append({
                    'index': i,
                    'state': 'anchoring',
                    'duration': low_speed_duration
                })
        elif speed > 3.0:  # èˆªè¡Œé˜ˆå€¼
            # æ£€æŸ¥èˆªå‘ä¸€è‡´æ€§
            course_stability = self._calculate_course_stability(df, i)
            if course_stability > 0.8:  # é«˜ç¨³å®šæ€§
                states.append({
                    'index': i,
                    'state': 'sailing',
                    'course_stability': course_stability
                })
```

### 3. Scene Aggregation (`scene_aggregator.py`)

åœºæ™¯èšåˆå™¨åˆ†æå¤šèˆ¹èˆ¶äº¤äº’å¹¶åˆ†ç±»å¯¼èˆªåœºæ™¯ã€‚

**åœºæ™¯åˆ†æçš„æ—¶é—´åˆ’åˆ†:**
- **æ—¶é—´çª—å£**: å¯é…ç½®çš„åˆ†æçª—å£ (é»˜è®¤: 600ç§’)
- **æ»‘åŠ¨çª—å£**: é‡å åˆ†æä»¥è¿›è¡Œè¿ç»­ç›‘æ§
- **äº‹ä»¶é©±åŠ¨åˆ†æ®µ**: åŸºäºäº¤äº’äº‹ä»¶çš„åˆ†æ®µ

```python
# æ—¶é—´åœºæ™¯åˆ›å»º (lines 150-200)
def _create_temporal_scenes(self, df: pd.DataFrame) -> List[Dict]:
    scenes = []

    # åˆ›å»ºæ»‘åŠ¨æ—¶é—´çª—å£
    start_time = df['timestamp'].min()
    end_time = df['timestamp'].max()
    current_time = start_time

    while current_time < end_time:
        window_end = current_time + pd.Timedelta(seconds=self.scene_window_size)

        # è·å–å½“å‰æ—¶é—´çª—å£ä¸­çš„èˆ¹èˆ¶
        window_data = df[
            (df['timestamp'] >= current_time) &
            (df['timestamp'] < window_end)
        ]

        if len(window_data) > 0:
            scene = self._analyze_scene(window_data, current_time)
            scenes.append(scene)

        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£
        current_time += pd.Timedelta(seconds=self.scene_step_size)

    return scenes
```

### 4. TCPA/DCPA Calculator (`tcpa_dcpa_calculator.py`)

ä½¿ç”¨å‘é‡åŒ–æ“ä½œè®¡ç®—ç¢°æ’é£é™©æŒ‡æ ‡ä»¥æé«˜æ•ˆç‡ã€‚

**æ—¶é—´é£é™©è¯„ä¼°:**
- **é¢„æµ‹æ—¶é—´è·¨åº¦**: å¯é…ç½®çš„æ—¶é—´èŒƒå›´ (é»˜è®¤: 1800ç§’)
- **é£é™©æ›´æ–°**: åœ¨æ¯ä¸ªæ—¶é—´æ­¥è¿ç»­è®¡ç®—é£é™©
- **åŠ¨æ€é‚»æ¥**: åŸºäºé‚»è¿‘åº¦æ›´æ–°èˆ¹èˆ¶å…³ç³»

```python
# å‘é‡åŒ–TCPA/DCPAè®¡ç®— (lines 120-160)
def calculate_tcpa_dcpa_vectorized(self, positions: np.ndarray,
                                  velocities: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¤šèˆ¹èˆ¶å¯¹çš„å‘é‡åŒ–TCPA/DCPAè®¡ç®—

    Args:
        positions: å½¢çŠ¶ (n_vessels, 2) - [x, y] ä½ç½®
        velocities: å½¢çŠ¶ (n_vessels, 2) - [vx, vy] é€Ÿåº¦

    Returns:
        tcpa: å½¢çŠ¶ (n_vessels, n_vessels) - CPAæ—¶é—´çŸ©é˜µ
        dcpa: å½¢çŠ¶ (n_vessels, n_vessels) - CPAè·ç¦»çŸ©é˜µ
    """
    n_vessels = positions.shape[0]

    # ç›¸å¯¹ä½ç½®å’Œé€Ÿåº¦
    rel_pos = positions[:, np.newaxis, :] - positions[np.newaxis, :, :]
    rel_vel = velocities[:, np.newaxis, :] - velocities[np.newaxis, :, :]

    # TCPAè®¡ç®—
    rel_speed_sq = np.sum(rel_vel**2, axis=2)
    rel_pos_dot_vel = np.sum(rel_pos * rel_vel, axis=2)

    tcpa = np.where(
        rel_speed_sq > 1e-6,
        -rel_pos_dot_vel / rel_speed_sq,
        np.inf
    )

    # DCPAè®¡ç®—
    dcpa = np.sqrt(np.sum(rel_pos**2, axis=2) +
                   2 * tcpa * rel_pos_dot_vel +
                   tcpa**2 * rel_speed_sq)

    return tcpa, dcpa
```

### 5. Scene Dataset (`scene_dataset.py`)

ç”¨äºé«˜æ•ˆæ¨¡å‹è®­ç»ƒçš„PyTorchæ•°æ®é›†å®ç°ã€‚

**æ•°æ®ç»„ç»‡:**
- **æ—¶é—´åºåˆ—**: å°†æ•°æ®ç»„ç»‡æˆåŸºäºæ—¶é—´çš„åºåˆ—
- **ç‰¹å¾æå–**: ä¸ºæ¨¡å‹è¾“å…¥æå–ç›¸å…³ç‰¹å¾
- **åŠ¨æ€æ‰¹å¤„ç†**: å¤„ç†å¯å˜é•¿åº¦åºåˆ—

```python
# PyTorchæ•°æ®é›†é¡¹è·å– (lines 180-220)
def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
    scene = self.scenes[idx]

    # æå–æ—¶é—´ç‰¹å¾
    history_features = self._extract_history_features(scene)
    future_features = self._extract_future_features(scene)
    static_features = self._extract_static_features(scene)

    # æ„å»ºé‚»æ¥çŸ©é˜µ
    adjacency_matrix = self._build_adjacency_matrix(scene)

    return {
        'history': torch.tensor(history_features, dtype=torch.float32),
        'future': torch.tensor(future_features, dtype=torch.float32),
        'static': torch.tensor(static_features, dtype=torch.float32),
        'adjacency': torch.tensor(adjacency_matrix, dtype=torch.float32),
        'scene_id': scene['scene_id']
    }
```

## Usage Examples

### Advanced Trajectory Analysis

```python
from ais_dstgt.data.preprocessing.trajectory_segmenter import TrajectorySegmenter

# é…ç½®åˆ†æ®µå‚æ•°
segmenter = TrajectorySegmenter(
    speed_change_threshold=2.0,  # èŠ‚
    course_change_threshold=30.0,  # åº¦
    min_segment_duration=300.0,  # ç§’
    port_radius=5000.0,  # ç±³
    min_port_duration=1800.0  # ç§’
)

# åˆ†æ®µè½¨è¿¹
segments = segmenter.segment_trajectories(df)

# åˆ†æèˆªæ¬¡æ¨¡å¼
voyage_analysis = segmenter.analyze_voyage_patterns(segments)
```

### Scene Analysis and Risk Assessment

```python
from ais_dstgt.data.preprocessing.scene_aggregator import SceneAggregator
from ais_dstgt.data.preprocessing.tcpa_dcpa_calculator import TCPADCPACalculator

# åˆå§‹åŒ–ç»„ä»¶
scene_aggregator = SceneAggregator(scene_window_size=600)
risk_calculator = TCPADCPACalculator(prediction_horizon=1800)

# åˆ†ææµ·äº‹åœºæ™¯
scenes = scene_aggregator.aggregate_scenes(df)
risk_graphs = risk_calculator.calculate_risk_graphs(scenes)

# è¯†åˆ«é«˜é£é™©åœºæ™¯
high_risk_scenes = [
    scene for scene in scenes
    if scene['risk_level'] > 0.7
]
```

## Development

### Prerequisites

- Python 3.12+
- Poetry for dependency management
- CUDA-compatible GPU (recommended)

### Setup Development Environment

```bash
# Clone repository
git clone https://github.com/EvanYzl/AIS_D-STGT.git
cd AIS_D-STGT

# Install dependencies
poetry install

# Install pre-commit hooks
poetry run pre-commit install

# Run tests
poetry run pytest

# Run linting
poetry run black .
poetry run isort .
poetry run ruff check .
poetry run mypy ais_dstgt
```

## Configuration

The framework supports extensive configuration through parameter settings:

```python
# Trajectory Smoother Configuration
smoother_config = {
    'process_noise_std': 1.0,
    'observation_noise_std': 10.0,
    'max_gap_duration': 3600.0,
    'outlier_threshold': 3.0
}

# Trajectory Segmenter Configuration
segmenter_config = {
    'speed_change_threshold': 2.0,
    'course_change_threshold': 30.0,
    'min_segment_duration': 300.0,
    'port_locations': {
        'Port_A': (40.7128, -74.0060),
        'Port_B': (51.5074, -0.1278)
    }
}

# Scene Aggregator Configuration
scene_config = {
    'scene_window_size': 600,
    'scene_step_size': 300,
    'interaction_distance': 10000.0,
    'congestion_threshold': 5
}
```

## Performance Metrics

The framework provides comprehensive quality metrics:

- **Trajectory Smoothing**: Position accuracy, velocity consistency, gap interpolation quality
- **Segmentation Quality**: Segment coherence, behavior classification accuracy
- **Scene Analysis**: Interaction detection rate, scene classification accuracy
- **Risk Assessment**: TCPA/DCPA calculation accuracy, collision prediction performance

## Dependencies

```
numpy>=1.21.0
pandas>=1.3.0
scipy>=1.7.0
torch>=1.9.0
geopy>=2.2.0
scikit-learn>=1.0.0
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Citation

If you use this work in your research, please cite:

```bibtex
@software{ais_dstgt,
  title={AIS Data Processing Framework: Advanced Maritime Trajectory Analysis},
  author={Evan Yzl},
  year={2024},
  url={https://github.com/EvanYzl/AIS_D-STGT}
}
```

## Support

- **Issues**: [GitHub Issues](https://github.com/EvanYzl/AIS_D-STGT/issues)
- **Discussions**: [GitHub Discussions](https://github.com/EvanYzl/AIS_D-STGT/discussions)

## Acknowledgments

- Maritime domain expertise from research publications
- AIS data processing methodologies from maritime safety organizations
- Deep learning frameworks for trajectory prediction research
   echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc && source ~/.bashrc
   ```
2. å…‹éš†ä»£ç å¹¶å®‰è£…ä¾èµ–
   ```bash
   git clone https://github.com/EvanYzl/AIS_D-STGT.git
   cd AIS_D-STGT
   # ä»…æ¸…æ´—åŠŸèƒ½
   poetry install --without dev,docs
   # è‹¥éœ€æŠ•å½±ã€å¡å°”æ›¼ã€å¼‚å¸¸æ£€æµ‹
   poetry add pyproj filterpy scikit-learn pyarrow
   ```

### 2. ç›®å½•çº¦å®š
```
AIS_D-STGT/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/        # åŸå§‹æ•°æ® (CSV)
â”‚   â”œâ”€â”€ processed/  # æ¸…æ´—åæ•°æ®
â”‚   â””â”€â”€ external/   # å‚è€ƒæ–‡æ¡£
â””â”€â”€ ais_dstgt/      # ä»£ç 
```

### 3. å•æ–‡ä»¶å¤„ç†ç¤ºä¾‹
```python
from ais_dstgt.data import AISDataProcessor

processor = AISDataProcessor(
    output_dir="data/processed",
    enable_coordinate_transform=False,   # ä»…æ¸…æ´—
    enable_kalman_filter=False,
    enable_anomaly_detection=False
)
processor.process_file(
    "data/raw/AIS_2024_01_01.csv",
    output_file="data/processed/AIS_2024_01_01.parquet",
    chunk_size=500_000
)
```

### 4. å¯ç”¨é«˜çº§åŠŸèƒ½
```python
processor = AISDataProcessor(
    output_dir="data/processed",
    enable_coordinate_transform=True,    # pyproj
    enable_kalman_filter=True,           # filterpy
    enable_anomaly_detection=True        # scikit-learn
)
processor.process_file("data/raw/AIS_2024_01_01.csv")
```

### 5. å¤šæ–‡ä»¶æ‰¹å¤„ç†
```python
processor = AISDataProcessor(output_dir="data/processed")
processor.process_directory("data/raw", file_pattern="*.csv", combine_files=True)
```

### 6. åˆ†åŒºå­˜å‚¨
```python
from ais_dstgt.data import AISDataProcessor, AISDataFrame
from pathlib import Path

ais_df = AISDataFrame.read_parquet("data/processed/AIS_2024_01_01.parquet")
processor = AISDataProcessor()
processor.create_data_partitions(
    ais_df, partition_by="date", output_dir=Path("data/processed/partitions")
)
```

### 7. æŠ¥å‘Šæ–‡ä»¶
æµæ°´çº¿ä¼šç”Ÿæˆ `processing_report.json`ï¼ŒåŒ…å«å„æ­¥éª¤è®°å½•æ•°ã€å¼‚å¸¸ç»Ÿè®¡ã€è¿‡æ»¤è´¨é‡ç­‰ä¿¡æ¯ï¼Œä¾¿äºå®¡è®¡ã€‚

### 8. å¸¸è§é—®é¢˜
| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
| ---- | -------- |
| ImportError: pyproj/filterpy/scikit-learn | `poetry add` å®‰è£…æˆ–åœ¨æ„é€  `AISDataProcessor` æ—¶ç¦ç”¨ç›¸å…³åŠŸèƒ½ |
| MemoryError | è°ƒå° `chunk_size`ï¼Œæˆ–ä½¿ç”¨ Parquet åˆ†åŒº |
| BaseDateTime è§£æå¤±è´¥ | ç¡®ä¿æ—¶é—´åˆ—ä¸º ISO-8601 æ ¼å¼ï¼Œæˆ–åœ¨ `CSVIngestionHandler` ä¼ å…¥ `parse_dates` å‚æ•° |

> ğŸ“Œ å®Œæ•´ç¤ºä¾‹è„šæœ¬è§ `examples/process_ais_data.py`ï¼Œæ”¯æŒ CLI ä¸€é”®è¿è¡Œã€‚
